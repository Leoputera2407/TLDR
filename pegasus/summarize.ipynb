{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pegasus.params import estimator_utils\n",
    "import tfds\n",
    "\n",
    "model_dir = 'ckpt/pegasus_ckpt/'\n",
    "model_params =  'fine_tune\"\n",
    "estimator = estimator_utils.create_estimator(\"\", model_dir, True, 1000, 1,\n",
    "                                                 model_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_CHKPT = 'ckpt/pegasus_ckpt/c4.unigram.newline.10pct.96000.model'\n",
    "encoder = public_parsing_ops.create_text_encoder(\"sentencepiece\", \n",
    "                                                     VOCAB_CHKPT)\n",
    "\n",
    "def input_fn(params):\n",
    "    file_paths = 'facebook_2015'\n",
    "    f = open(f\"{file_path}.txt\",\"r\")\n",
    "    full_text = f.read()\n",
    "    # Break into chunks of 1024 tokens\n",
    "    # Fully aware that they tokenize on BPE Encoding, so chunking on whole words may results in a small amount of lost\n",
    "    # tokens, which is fine to me.\n",
    "    text = full_text.split()\n",
    "    n = 1024\n",
    "    inputs = [encoder.encode(' '.join(text[i:i+n])) for i in range(0,len(text),n)]\n",
    "    #To generate new summaries, pass empty strings\n",
    "    targets = [encoder.encoder('') for i in range(len(inputs))]\n",
    "    \n",
    "    \n",
    "    \n",
    "    input_dict = dict(\n",
    "                  inputs=inputs,\n",
    "                  targets=targets\n",
    "                 )\n",
    "\n",
    "    data = pd.DataFrame(input_dict)\n",
    "\n",
    "    with tf.io.TFRecordWriter('evaluate_data.tfrecords') as writer:\n",
    "        for row in data.values:\n",
    "            inputs, targets = row[:-1], row[-1]\n",
    "            example = tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={\n",
    "                    \"inputs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[inputs[0].encode('utf-8')])),\n",
    "                    \"targets\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[targets.encode('utf-8')])),\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        writer.write(example.SerializeToString())\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(filenames = ['evaluate_data.tfrecords'])\n",
    "    return ds\n",
    "    \n",
    "checkpoint_path = 'ckpt/pegasus_ckpt/fine_tune.model'\n",
    "predictions = estimator.predict(\n",
    "          input_fn=input_fn, checkpoint_path=checkpoint_path)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
