{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/halim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 112 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: spacy.tokens.span.Span size changed, may indicate binary incompatibility. Expected 72 from C header, got 80 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('popular')\n",
    "import pprint\n",
    "import itertools\n",
    "import re\n",
    "import pke\n",
    "import string\n",
    "from summarizer import Summarizer\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Generic\n",
    "import spacy\n",
    "#https://spacy.io/universe/project/neuralcoref\n",
    "import neuralcoref\n",
    "import pandas as pd\n",
    "import pdftotext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize using bert-distill-summarizer\n",
    "Tried to summarize with T5 and BART models too (check out Alt-summarizer.ipynb); however, they don't summarize as well, based on the Rouge-L score. This is still better.\n",
    "\n",
    "UPDATE 1: Managed to Fine-tune Pegasus (current SoTa) with a small legal summarization (in plain english) dataset. Although, the summaries are better than Bert, but the model is very very slow (more than 10 minutes on 2070 Ti). Moreover, Pegasus can only summarize on a maximum of 1024 tokens (it'll discard any tokens more than that). This, I feel, kind of defeats the purpose? So, I'll just stick with Bert here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"facebook_2015.txt\",\"r\")\n",
    "full_text = f.read()\n",
    "\n",
    "model = Summarizer()\n",
    "result = model(full_text, min_length=60, max_length = 500 , ratio = 0.4)\n",
    "\n",
    "summarized_text = ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['facebook principles', 'facebook', 'statement', 'rights', 'responsibilities', 'srr', 'facebook services', 'platform', 'special provisions', 'california', 'state', 'account security', 'united states', 'services', 'commercial content', 'page', 'applications', 'safe', 'advertisements', 'data policy']\n",
      "['facebook principles', 'facebook', 'statement', 'rights', 'responsibilities', 'srr', 'facebook services', 'platform', 'special provisions', 'california', 'state', 'united states', 'services', 'commercial content', 'page', 'safe', 'advertisements', 'data policy']\n"
     ]
    }
   ],
   "source": [
    "def get_nouns_multipartite(text):\n",
    "    out=[]\n",
    "\n",
    "    extractor = pke.unsupervised.MultipartiteRank()\n",
    "    extractor.load_document(input=text)\n",
    "    #    not contain punctuation marks or stopwords as candidates.\n",
    "    pos = {'PROPN'}\n",
    "    #pos = {'VERB', 'ADJ', 'NOUN'}\n",
    "    stoplist = list(string.punctuation)\n",
    "    stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "    stoplist += stopwords.words('english')\n",
    "    extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "    # 4. build the Multipartite graph and rank candidates using random walk,\n",
    "    #    alpha controls the weight adjustment mechanism, see TopicRank for\n",
    "    #    threshold/method parameters.\n",
    "    extractor.candidate_weighting(alpha=1.1,\n",
    "                                  threshold=0.75,\n",
    "                                  method='average')\n",
    "    keyphrases = extractor.get_n_best(n=20)\n",
    "\n",
    "    for key in keyphrases:\n",
    "        out.append(key[0])\n",
    "\n",
    "    return out\n",
    "\n",
    "keywords = get_nouns_multipartite(full_text) \n",
    "print (keywords)\n",
    "filtered_keys=[]\n",
    "for keyword in keywords:\n",
    "    if keyword.lower() in summarized_text.lower():\n",
    "        filtered_keys.append(keyword)\n",
    "        \n",
    "print (filtered_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above approximates how lossy our summaries are\n",
    "The code above essentially ranks the importance of each words using TopicRank. It seems we capture most topics discussed without dropping too many important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do some pre-processing\n",
    "\"\"\"\n",
    "paragraphs = summarized_text.split(\"\\n\\n\")\n",
    "cleaned_sentences=[]\n",
    "for paragraph in paragraphs:\n",
    "    for sent in sent_tokenize(paragraph):\n",
    "        lower_token = [word.lower() for word in sent.split()]\n",
    "        lower_cased = \" \".join([word for word in lower_token])\n",
    "        punct_removed = re.sub(r'[^\\w\\s]','',lower_cased)\n",
    "        if len(punct_removed.split()) > 2:\n",
    "            cleaned_sentences.append(punct_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank the importance of each sentence by using page-rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /var/folders/3l/0t74styx5wz0y5tkfgsvhycwfc38wl/T/tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading embeddings\n",
      "Done downloading\n",
      "Sentence Importances :\n",
      "n°0 : we do not give your content or information to advertisers without your consent\n",
      "n°1 : by content we mean anything you or other users post provide or share using facebook services\n",
      "n°2 : you will not post content or take any action on facebook that infringes or violates someone elses rights or otherwise violates the law\n",
      "n°3 : although we provide rules for user conduct we do not control or direct users actions on facebook and are not responsible for the content or information users transmit or share on facebook\n",
      "n°4 : you own all of the content and information you post on facebook and you can control how it is shared through your privacy and application settings\n",
      "n°5 : we designed our data policy to make important disclosures about how you can use facebook to share with others and how we collect and can use your content and information\n",
      "n°6 : when you publish content or information using the public setting it means that you are allowing everyone including people off of facebook to access and use that information and to associate it with you ie your name and profile picture\n",
      "n°7 : in order to help us do that you agree to the following you give us permission to use your name profile picture content and information in connection with commercial sponsored or related content such as a brand you like served or enhanced by us\n",
      "n°8 : this means for example that you permit a business or other entity to pay us to display your name andor profile picture with your content or information without any compensation to you\n",
      "n°9 : statement of rights and responsibilities this statement of rights and responsibilities statement terms or srr derives from the facebook principles and is our terms of service that governs our relationship with users and others who interact with facebook as well as facebook brands products and services which we call the facebook services or services\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading embeddings\")\n",
    "url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "embeddings = hub.load(url)\n",
    "print(\"Done downloading\")\n",
    "\n",
    "sent_len = len(cleaned_sentences)\n",
    "sent_embed = embeddings(cleaned_sentences)\n",
    "\n",
    "similarity_matrix = np.zeros([sent_len, sent_len])\n",
    "for i in range(sent_len):\n",
    "    for j in range(sent_len):\n",
    "        if i != j:\n",
    "            similarity_matrix[i][j] = cosine_similarity([sent_embed[i].numpy()], [sent_embed[j].numpy()])\n",
    "\n",
    "graph = nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(graph)\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(cleaned_sentences)), reverse=True)\n",
    "#Ranked top 10 important sentences using page rank\n",
    "TOP_N_IMPT = 10\n",
    "top_n_sent = [ranked_sentences[i][1] for i in range(TOP_N_IMPT)]\n",
    "\n",
    "print(\"Sentence Importances :\")\n",
    "for i, sent in enumerate(top_n_sent):\n",
    "    print(\"n°%d : %s\" % (i, sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's start our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SummarizeClass:\n",
    "    sent:str\n",
    "    model: Summarizer\n",
    "    embed: Generic\n",
    "    summarized_text:str = None\n",
    "    sentences: List[str] = None\n",
    "    \n",
    "    \n",
    "    def summarize(self):\n",
    "        self.summarized_text = model(self.sent, min_length=60, max_length = 500 , ratio = 0.4)\n",
    "    \n",
    "    def clean_sent(self):\n",
    "        paragraphs = self.summarized_text.split(\"\\n\\n\")\n",
    "        cleaned_sentences=[]\n",
    "        for paragraph in paragraphs:\n",
    "            for sent in sent_tokenize(paragraph):\n",
    "                lower_token = [word.lower() for word in sent.split()]\n",
    "                lower_cased = \" \".join([word for word in lower_token])\n",
    "                punct_removed = re.sub(r'[^\\w\\s]','',lower_cased)\n",
    "                if len(punct_removed.split()) > 2:\n",
    "                    cleaned_sentences.append(punct_removed)\n",
    "        self.sentences = cleaned_sentences\n",
    "    def rank_sent(self, TOP_N_IMPT):\n",
    "        sent_len = len(self.sentences)\n",
    "        sent_embed = self.embed(self.sentences)\n",
    "        similarity_matrix = np.zeros([sent_len, sent_len])\n",
    "        for i in range(sent_len):\n",
    "            for j in range(sent_len):\n",
    "                if i != j:\n",
    "                    similarity_matrix[i][j] = cosine_similarity([sent_embed[i].numpy()], [sent_embed[j].numpy()])\n",
    "\n",
    "        graph = nx.from_numpy_array(similarity_matrix)\n",
    "        scores = nx.pagerank(graph)\n",
    "        ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(self.sentences)), reverse=True)\n",
    "        #Ranked top 10 important sentences using page rank\n",
    "        top_n_sent = [ranked_sentences[i][1] for i in range(TOP_N_IMPT)]\n",
    "        return top_n_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /var/folders/3l/0t74styx5wz0y5tkfgsvhycwfc38wl/T/tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "model = Summarizer()\n",
    "url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "embeddings = hub.load(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 sentences for facebook_2015\n",
      "SENTENCES :\n",
      "n°0 : we do not give your content or information to advertisers without your consent\n",
      "n°1 : by content we mean anything you or other users post provide or share using facebook services\n",
      "n°2 : you will not post content or take any action on facebook that infringes or violates someone elses rights or otherwise violates the law\n",
      "n°3 : although we provide rules for user conduct we do not control or direct users actions on facebook and are not responsible for the content or information users transmit or share on facebook\n",
      "n°4 : you own all of the content and information you post on facebook and you can control how it is shared through your privacy and application settings\n",
      "n°5 : we designed our data policy to make important disclosures about how you can use facebook to share with others and how we collect and can use your content and information\n",
      "n°6 : when you publish content or information using the public setting it means that you are allowing everyone including people off of facebook to access and use that information and to associate it with you ie your name and profile picture\n",
      "n°7 : in order to help us do that you agree to the following you give us permission to use your name profile picture content and information in connection with commercial sponsored or related content such as a brand you like served or enhanced by us\n",
      "n°8 : this means for example that you permit a business or other entity to pay us to display your name andor profile picture with your content or information without any compensation to you\n",
      "n°9 : statement of rights and responsibilities this statement of rights and responsibilities statement terms or srr derives from the facebook principles and is our terms of service that governs our relationship with users and others who interact with facebook as well as facebook brands products and services which we call the facebook services or services\n",
      "---------\n",
      "The top 10 sentences for facebook_2018\n",
      "SENTENCES :\n",
      "n°0 : other terms and policies that may apply to you community standards these guidelines outline our standards regarding the content you post to facebook and your activity on facebook and other facebook products\n",
      "n°1 : these terms govern your use of facebook and the products features apps services technologies and software we offer the facebook products or products except where we expressly state that separate terms and not these apply\n",
      "n°2 : specifically when you share post or upload content that is covered by intellectual property rights like photos or videos on or in connection with our products you grant us a nonexclusive transferable sublicensable royaltyfree and worldwide license to host use distribute modify run copy publicly perform or display translate and create derivative works of your content consistent with your privacy and application settings\n",
      "n°3 : to help support our community we encourage you to report content or conduct that you believe violates your rights including intellectual property rights or our terms and policies\n",
      "n°4 : you can only use our copyrights or trademarks or any similar marks as expressly permitted by our brand usage guidelines or with our prior written permission\n",
      "n°5 : if you use any of those products supplemental terms will be made available and will become part of our agreement with you\n",
      "n°6 : help you discover content products and services that may interest you we show you ads offers and other sponsored content to help you discover content products and services that are offered by the many businesses and organizations that use facebook and other facebook products\n",
      "n°7 : we share data with other facebook companies when we detect misuse or harmful conduct by someone using one of our products\n",
      "n°8 : once any updated terms are in effect you will be bound by them if you continue to use our products\n",
      "n°9 : we design our systems so that your experience is consistent and seamless across the different facebook company products that you use\n",
      "---------\n",
      "The top 10 sentences for facebook_jul_2019\n",
      "SENTENCES :\n",
      "n°0 : help you discover content products and services that may interest you we show you ads offers and other sponsored content to help you discover content products and services that are offered by the many businesses and organizations that use facebook and other facebook products\n",
      "n°1 : other terms and policies that may apply to you community standards these guidelines outline our standards regarding the content you post to facebook and your activity on facebook and other facebook products\n",
      "n°2 : our data policy explains how we collect and use your personal data to determine some of the ads you see and provide all of the other services described below\n",
      "n°3 : permission to use your name profile picture and information about your actions with ads and sponsored content you give us permission to use your name and profile picture and information about actions you have taken on facebook next to or in connection with ads offers and other sponsored content that we display across our products without any compensation to you\n",
      "n°4 : if you use any of those products supplemental terms will be made available and will become part of our agreement with you\n",
      "n°5 : if we determine that you have clearly seriously or repeatedly breached our terms or policies including in particular our community standards we may suspend or permanently disable access to your account\n",
      "n°6 : your commitments to facebook and our community we provide these services to you and others to help advance our mission\n",
      "n°7 : we share data with other facebook companies when we detect misuse or harmful conduct by someone using one of our products\n",
      "n°8 : once any updated terms are in effect you will be bound by them if you continue to use our products\n",
      "n°9 : instead businesses and organizations pay us to show you ads for their products and services\n",
      "---------\n",
      "The top 10 sentences for facebook_dec_2019\n",
      "SENTENCES :\n",
      "n°0 : if you use any of those products supplemental terms will be made available and will become part of our agreement with you\n",
      "n°1 : this means that we can show you relevant and useful ads without telling advertisers who you are\n",
      "n°2 : once any updated terms are in effect you will be bound by them if you continue to use our products\n",
      "n°3 : specifically when you share post or upload content that is covered by intellectual property rights on or in connection with our products you grant us a nonexclusive transferable sublicensable royaltyfree and worldwide license to host use distribute modify run copy publicly perform or display translate and create derivative works of your content consistent with your privacy and application settings\n",
      "n°4 : if we determine that you have clearly seriously or repeatedly breached our terms or policies including in particular our community standards we may suspend or permanently disable access to your account\n",
      "n°5 : help you discover content products and services that may interest you we show you ads offers and other sponsored content to help you discover content products and services that are offered by the many businesses and organizations that use facebook and other facebook products\n",
      "n°6 : if you delete or we disable your account these terms shall terminate as an agreement between you and us but the following provisions will remain in place 3 4245\n",
      "n°7 : instead businesses and organizations pay us to show you ads for their products and services\n",
      "n°8 : you will not transfer any of your rights or obligations under these terms to anyone else without our consent\n",
      "n°9 : you can only use our copyrights or trademarks or any similar marks as expressly permitted by our brand usage guidelines or with our prior written permission\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "file_paths = ['facebook_2015', 'facebook_2018', 'facebook_jul_2019','facebook_dec_2019', 'Robinhood', 'patreon' ]\n",
    "full_texts =[]\n",
    "outputs =[]\n",
    "for file_path in file_paths:\n",
    "    f = open(f\"{file_path}.txt\",\"r\")\n",
    "    full_text = f.read()\n",
    "    full_texts.append(full_text)\n",
    "    Task = SummarizeClass(sent=full_text, model = model, embed = embeddings)\n",
    "    Task.summarize()\n",
    "    Task.clean_sent()\n",
    "    TOP_N_IMPT = 10\n",
    "    output = Task.rank_sent(TOP_N_IMPT=TOP_N_IMPT)\n",
    "    outputs.append(output)\n",
    "    print(f\"The top {TOP_N_IMPT} sentences for {file_path}\")\n",
    "    print(\"SENTENCES :\")\n",
    "    for i, sent in enumerate(output):\n",
    "        print(\"n°%d : %s\" % (i, sent))\n",
    "    print(\"---------\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nlp = spacy.load('en')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "\n",
    "def entity_pairs(text, coref=True):\n",
    "    text = re.sub(r'\\n+', '.', text)  # replace multiple newlines with period\n",
    "    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers\n",
    "    text = nlp(text)\n",
    "    ent_pairs = list()\n",
    "    if coref:\n",
    "        text = nlp(text._.coref_resolved)  # resolve coreference clusters\n",
    "    sentences = [sent.string.strip() for sent in text.sents]  # split text into sentences\n",
    "    for sent in sentences:\n",
    "        sent = nlp(sent)\n",
    "        spans = list(sent.ents) + list(sent.noun_chunks)  # collect nodes\n",
    "        spans = spacy.util.filter_spans(spans)\n",
    "        with sent.retokenize() as retokenizer:\n",
    "            [retokenizer.merge(span) for span in spans]\n",
    "        dep = [token.dep_ for token in sent]\n",
    "        if (dep.count('obj')+dep.count('dobj'))>=1 or (dep.count('subj')+dep.count('nsubj'))>=1:\n",
    "            for token in sent:\n",
    "                if token.dep_ in ('obj', 'dobj'):  # identify object nodes\n",
    "                    subject = [w for w in token.head.lefts if w.dep_\n",
    "                               in ('subj', 'nsubj')]  # identify subject nodes\n",
    "                    if subject:\n",
    "                        subject = subject[0]\n",
    "                        # identify relationship by root dependency\n",
    "                        relation = [w for w in token.ancestors if w.dep_ == 'ROOT']  \n",
    "                        if relation:\n",
    "                            relation = relation[0]\n",
    "                            # add adposition or particle to relationship\n",
    "                            if relation.nbor(1).pos_ in ('ADP', 'PART'):  \n",
    "                                relation = ' '.join((str(relation),\n",
    "                                        str(relation.nbor(1))))\n",
    "                        else:\n",
    "                            relation = 'unknown'\n",
    "                        subject, subject_type = refine_ent(subject, sent)\n",
    "                        token, object_type = refine_ent(token, sent)\n",
    "                        ent_pairs.append([str(subject), str(relation), str(token),\n",
    "                                str(subject_type), str(object_type)])\n",
    "    filtered_ent_pairs = [sublist for sublist in ent_pairs\n",
    "                          if not any(str(x) == '' for x in sublist)]\n",
    "    pairs = pd.DataFrame(filtered_ent_pairs, columns=['subject',\n",
    "                         'relation', 'object', 'subject_type',\n",
    "                         'object_type'])\n",
    "    print('Entity pairs extracted:', str(len(filtered_ent_pairs)))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def refine_ent(ent, sent):\n",
    "    unwanted_tokens = (\n",
    "        'PRON',  # pronouns\n",
    "        'PART',  # particle\n",
    "        'DET',  # determiner\n",
    "        'SCONJ',  # subordinating conjunction\n",
    "        'PUNCT',  # punctuation\n",
    "        'SYM',  # symbol\n",
    "        'X',  # other\n",
    "        )\n",
    "    ent_type = ent.ent_type_  # get entity type\n",
    "    if ent_type == '':\n",
    "        ent_type = 'NOUN_CHUNK'\n",
    "        ent = ' '.join(str(t.text) for t in\n",
    "                nlp(str(ent)) if t.pos_\n",
    "                not in unwanted_tokens and t.is_stop == False)\n",
    "    elif ent_type in ('NOMINAL', 'CARDINAL', 'ORDINAL') and str(ent).find(' ') == -1:\n",
    "        t = ''\n",
    "        for i in range(len(sent) - ent.i):\n",
    "            if ent.nbor(i).pos_ not in ('VERB', 'PUNCT'):\n",
    "                t += ' ' + str(ent.nbor(i))\n",
    "            else:\n",
    "                ent = t.strip()\n",
    "                break\n",
    "    return ent, ent_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_kg(pairs):\n",
    "    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n",
    "            create_using=nx.MultiDiGraph())\n",
    "    node_deg = nx.degree(k_graph)\n",
    "    layout = nx.spring_layout(k_graph, k=0.15, iterations=20)\n",
    "    plt.figure(num=None, figsize=(30, 10), dpi=100)\n",
    "    nx.draw_networkx(\n",
    "        k_graph,\n",
    "        node_size=[int(deg[1]) * 1000 for deg in node_deg],\n",
    "        arrowsize=20,\n",
    "        linewidths=1.5,\n",
    "        pos=layout,\n",
    "        edge_color='red',\n",
    "        edgecolors='black',\n",
    "        node_color='white',\n",
    "        )\n",
    "    labels = dict(zip(list(zip(pairs.subject, pairs.object)),\n",
    "                  pairs['relation'].tolist()))\n",
    "    nx.draw_networkx_edge_labels(k_graph, pos=layout, edge_labels=labels,\n",
    "                                 font_color='red')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "for idx,text in enumerate(full_texts):\n",
    "    pairs = entity_pairs(text)\n",
    "    file_name = file_paths[idx]\n",
    "    print(f\"Knowledge graph for {file_name}\")\n",
    "    draw_kg(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summ",
   "language": "python",
   "name": "summ"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
