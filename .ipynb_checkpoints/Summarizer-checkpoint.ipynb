{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/halim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/halim/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('popular')\n",
    "import pprint\n",
    "import itertools\n",
    "import re\n",
    "import pke\n",
    "import string\n",
    "from summarizer import Summarizer\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Generic\n",
    "import spacy\n",
    "#https://spacy.io/universe/project/neuralcoref\n",
    "import neuralcoref\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize using bert-distill-summarizer\n",
    "Tried to summarize with T5 and BART models too (check out Alt-summarizer.ipynb); however, they don't summarize as well, based on the Rouge-L score. This is still better.\n",
    "\n",
    "UPDATE 1: Managed to Fine-tune Pegasus (current SoTa) with a small legal summarization (in plain englosh dataset). Although, the summaries are better than Bert, but the model is very very slow (more than 10 minutes on 2070 Ti). Moreover, Pegasus can only summarize on a maximum of 1024 tokens (it'll discard any tokens more than that). This I feel kind of defeats the purpose? So, I'll just stick with Bert here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"facebook_2015.txt\",\"r\")\n",
    "full_text = f.read()\n",
    "\n",
    "model = Summarizer()\n",
    "result = model(full_text, min_length=60, max_length = 500 , ratio = 0.4)\n",
    "\n",
    "summarized_text = ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['facebook principles', 'facebook', 'statement', 'responsibilities', 'srr', 'rights', 'special provisions', 'platform', 'facebook services', 'california', 'state', 'limited', 'applications', 'damages', 'data policy', 'commercial content served', 'united states', 'known', 'implied warranties including', 'account security']\n",
      "['facebook principles', 'facebook', 'statement', 'responsibilities', 'srr', 'rights', 'special provisions', 'platform', 'facebook services', 'california', 'state', 'limited', 'data policy', 'commercial content served', 'united states']\n"
     ]
    }
   ],
   "source": [
    "def get_nouns_multipartite(text):\n",
    "    out=[]\n",
    "\n",
    "    extractor = pke.unsupervised.MultipartiteRank()\n",
    "    extractor.load_document(input=text)\n",
    "    #    not contain punctuation marks or stopwords as candidates.\n",
    "    pos = {'PROPN'}\n",
    "    #pos = {'VERB', 'ADJ', 'NOUN'}\n",
    "    stoplist = list(string.punctuation)\n",
    "    stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "    stoplist += stopwords.words('english')\n",
    "    extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "    # 4. build the Multipartite graph and rank candidates using random walk,\n",
    "    #    alpha controls the weight adjustment mechanism, see TopicRank for\n",
    "    #    threshold/method parameters.\n",
    "    extractor.candidate_weighting(alpha=1.1,\n",
    "                                  threshold=0.75,\n",
    "                                  method='average')\n",
    "    keyphrases = extractor.get_n_best(n=20)\n",
    "\n",
    "    for key in keyphrases:\n",
    "        out.append(key[0])\n",
    "\n",
    "    return out\n",
    "\n",
    "keywords = get_nouns_multipartite(full_text) \n",
    "print (keywords)\n",
    "filtered_keys=[]\n",
    "for keyword in keywords:\n",
    "    if keyword.lower() in summarized_text.lower():\n",
    "        filtered_keys.append(keyword)\n",
    "        \n",
    "print (filtered_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above approximates how lossy our summaries are\n",
    "The code above essentially ranks the importance of each words using TopicRank. It seems we capture most topics discussed without dropping too many important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Do some pre-processing\n",
    "\"\"\"\n",
    "paragraphs = summarized_text.split(\"\\n\\n\")\n",
    "cleaned_sentences=[]\n",
    "for paragraph in paragraphs:\n",
    "    for sent in sent_tokenize(paragraph):\n",
    "        lower_token = [word.lower() for word in sent.split()]\n",
    "        lower_cased = \" \".join([word for word in lower_token])\n",
    "        punct_removed = re.sub(r'[^\\w\\s]','',lower_cased)\n",
    "        if len(punct_removed.split()) > 2:\n",
    "            cleaned_sentences.append(punct_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank the importance of each sentence by using page-rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /var/folders/3l/0t74styx5wz0y5tkfgsvhycwfc38wl/T/tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading embeddings\n",
      "Done downloading\n",
      "Sentence Importances :\n",
      "n°0 : we do not give your content or information to advertisers without your consent\n",
      "n°1 : by content we mean anything you or other users post provide or share using facebook services\n",
      "n°2 : you will not post content or take any action on facebook that infringes or violates someone elses rights or otherwise violates the law\n",
      "n°3 : although we provide rules for user conduct we do not control or direct users actions on facebook and are not responsible for the content or information users transmit or share on facebook\n",
      "n°4 : you own all of the content and information you post on facebook and you can control how it is shared through your privacy and application settings\n",
      "n°5 : we designed our data policy to make important disclosures about how you can use facebook to share with others and how we collect and can use your content and information\n",
      "n°6 : when you publish content or information using the public setting it means that you are allowing everyone including people off of facebook to access and use that information and to associate it with you ie your name and profile picture\n",
      "n°7 : in order to help us do that you agree to the following you give us permission to use your name profile picture content and information in connection with commercial sponsored or related content such as a brand you like served or enhanced by us\n",
      "n°8 : this means for example that you permit a business or other entity to pay us to display your name andor profile picture with your content or information without any compensation to you\n",
      "n°9 : statement of rights and responsibilities this statement of rights and responsibilities statement terms or srr derives from the facebook principles and is our terms of service that governs our relationship with users and others who interact with facebook as well as facebook brands products and services which we call the facebook services or services\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading embeddings\")\n",
    "url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "embeddings = hub.load(url)\n",
    "print(\"Done downloading\")\n",
    "\n",
    "sent_len = len(cleaned_sentences)\n",
    "sent_embed = embeddings(cleaned_sentences)\n",
    "\n",
    "similarity_matrix = np.zeros([sent_len, sent_len])\n",
    "for i in range(sent_len):\n",
    "    for j in range(sent_len):\n",
    "        if i != j:\n",
    "            similarity_matrix[i][j] = cosine_similarity([sent_embed[i].numpy()], [sent_embed[j].numpy()])\n",
    "\n",
    "graph = nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(graph)\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(cleaned_sentences)), reverse=True)\n",
    "#Ranked top 10 important sentences using page rank\n",
    "TOP_N_IMPT = 10\n",
    "top_n_sent = [ranked_sentences[i][1] for i in range(TOP_N_IMPT)]\n",
    "\n",
    "print(\"Sentence Importances :\")\n",
    "for i, sent in enumerate(top_n_sent):\n",
    "    print(\"n°%d : %s\" % (i, sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's start our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SummarizeClass:\n",
    "    sent:str\n",
    "    model: Summarizer\n",
    "    embed: Generic\n",
    "    summarized_text:str = None\n",
    "    sentences: List[str] = None\n",
    "    \n",
    "    \n",
    "    def summarize(self):\n",
    "        self.summarized_text = model(self.sent, min_length=60, max_length = 500 , ratio = 0.4)\n",
    "    \n",
    "    def clean_sent(self):\n",
    "        paragraphs = self.summarized_text.split(\"\\n\\n\")\n",
    "        cleaned_sentences=[]\n",
    "        for paragraph in paragraphs:\n",
    "            for sent in sent_tokenize(paragraph):\n",
    "                lower_token = [word.lower() for word in sent.split()]\n",
    "                lower_cased = \" \".join([word for word in lower_token])\n",
    "                punct_removed = re.sub(r'[^\\w\\s]','',lower_cased)\n",
    "                if len(punct_removed.split()) > 2:\n",
    "                    cleaned_sentences.append(punct_removed)\n",
    "        self.sentences = cleaned_sentences\n",
    "    def rank_sent(self, TOP_N_IMPT):\n",
    "        sent_len = len(self.sentences)\n",
    "        sent_embed = self.embed(self.sentences)\n",
    "        similarity_matrix = np.zeros([sent_len, sent_len])\n",
    "        for i in range(sent_len):\n",
    "            for j in range(sent_len):\n",
    "                if i != j:\n",
    "                    similarity_matrix[i][j] = cosine_similarity([sent_embed[i].numpy()], [sent_embed[j].numpy()])\n",
    "\n",
    "        graph = nx.from_numpy_array(similarity_matrix)\n",
    "        scores = nx.pagerank(graph)\n",
    "        ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(cleaned_sentences)), reverse=True)\n",
    "        #Ranked top 10 important sentences using page rank\n",
    "        top_n_sent = [ranked_sentences[i][1] for i in range(TOP_N_IMPT)]\n",
    "        return top_n_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summarizer()\n",
    "embeddings = hub.load(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important sentence for facebook_2015\n",
      "['we do not give your content or information to advertisers without your consent', 'by content we mean anything you or other users post provide or share using facebook services', 'you will not post content or take any action on facebook that infringes or violates someone elses rights or otherwise violates the law', 'although we provide rules for user conduct we do not control or direct users actions on facebook and are not responsible for the content or information users transmit or share on facebook', 'you own all of the content and information you post on facebook and you can control how it is shared through your privacy and application settings', 'we designed our data policy to make important disclosures about how you can use facebook to share with others and how we collect and can use your content and information', 'when you publish content or information using the public setting it means that you are allowing everyone including people off of facebook to access and use that information and to associate it with you ie your name and profile picture', 'in order to help us do that you agree to the following you give us permission to use your name profile picture content and information in connection with commercial sponsored or related content such as a brand you like served or enhanced by us', 'this means for example that you permit a business or other entity to pay us to display your name andor profile picture with your content or information without any compensation to you', 'statement of rights and responsibilities this statement of rights and responsibilities statement terms or srr derives from the facebook principles and is our terms of service that governs our relationship with users and others who interact with facebook as well as facebook brands products and services which we call the facebook services or services']\n",
      "Most important sentence for facebook_2019\n",
      "['by use we mean use run copy publicly perform or display distribute modify translate and create derivative works of', 'if we disable your account you will not create another one without our permission', 'although we provide rules for user conduct we do not control or direct users actions on facebook and are not responsible for the content or information users transmit or share on facebook', 'our goal is to deliver advertising and other commercial or sponsored content that is valuable to our users and advertisers', 'we do not guarantee that facebook will always be safe secure or errorfree or that facebook will always function without disruptions delays or imperfections', 'you will not collect users content or information or otherwise access facebook using automated means such as harvesting bots robots spiders or scrapers without our prior permission', 'special provisions applicable to users outside the united states', 'by using or accessing the facebook services you agree to this statement as updated from time to time in accordance with section 13 below', 'you will not transfer any of your rights or obligations under this statement to anyone else without our consent', 'you may also delete your account or disable your application at any time']\n"
     ]
    }
   ],
   "source": [
    "file_paths = ['facebook_2015', 'facebook_2019']\n",
    "full_texts =[]\n",
    "outputs =[]\n",
    "for file_path in file_paths:\n",
    "    f = open(f\"{file_path}.txt\",\"r\")\n",
    "    full_text = f.read()\n",
    "    full_texts.append(full_text)\n",
    "    Task = SummarizeClass(sent=full_text, model = model, embed = embeddings)\n",
    "    Task.summarize()\n",
    "    Task.clean_sent()\n",
    "    output = Task.rank_sent(TOP_N_IMPT=10)\n",
    "    outputs.append(output)\n",
    "    print(f\"Most important sentence for {file_path}\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nlp = spacy.load('en')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "\n",
    "def entity_pairs(text, coref=True):\n",
    "    text = re.sub(r'\\n+', '.', text)  # replace multiple newlines with period\n",
    "    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers\n",
    "    text = nlp(text)\n",
    "    ent_pairs = list()\n",
    "    if coref:\n",
    "        text = nlp(text._.coref_resolved)  # resolve coreference clusters\n",
    "    sentences = [sent.string.strip() for sent in text.sents]  # split text into sentences\n",
    "    for sent in sentences:\n",
    "        sent = nlp(sent)\n",
    "        spans = list(sent.ents) + list(sent.noun_chunks)  # collect nodes\n",
    "        spans = spacy.util.filter_spans(spans)\n",
    "        with sent.retokenize() as retokenizer:\n",
    "            [retokenizer.merge(span) for span in spans]\n",
    "        dep = [token.dep_ for token in sent]\n",
    "        if (dep.count('obj')+dep.count('dobj'))>=1 or (dep.count('subj')+dep.count('nsubj'))>=1:\n",
    "            for token in sent:\n",
    "                if token.dep_ in ('obj', 'dobj'):  # identify object nodes\n",
    "                    subject = [w for w in token.head.lefts if w.dep_\n",
    "                               in ('subj', 'nsubj')]  # identify subject nodes\n",
    "                    if subject:\n",
    "                        subject = subject[0]\n",
    "                        # identify relationship by root dependency\n",
    "                        relation = [w for w in token.ancestors if w.dep_ == 'ROOT']  \n",
    "                        if relation:\n",
    "                            relation = relation[0]\n",
    "                            # add adposition or particle to relationship\n",
    "                            if relation.nbor(1).pos_ in ('ADP', 'PART'):  \n",
    "                                relation = ' '.join((str(relation),\n",
    "                                        str(relation.nbor(1))))\n",
    "                        else:\n",
    "                            relation = 'unknown'\n",
    "                        subject, subject_type = refine_ent(subject, sent)\n",
    "                        token, object_type = refine_ent(token, sent)\n",
    "                        ent_pairs.append([str(subject), str(relation), str(token),\n",
    "                                str(subject_type), str(object_type)])\n",
    "    filtered_ent_pairs = [sublist for sublist in ent_pairs\n",
    "                          if not any(str(x) == '' for x in sublist)]\n",
    "    pairs = pd.DataFrame(filtered_ent_pairs, columns=['subject',\n",
    "                         'relation', 'object', 'subject_type',\n",
    "                         'object_type'])\n",
    "    print('Entity pairs extracted:', str(len(filtered_ent_pairs)))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def refine_ent(ent, sent):\n",
    "    unwanted_tokens = (\n",
    "        'PRON',  # pronouns\n",
    "        'PART',  # particle\n",
    "        'DET',  # determiner\n",
    "        'SCONJ',  # subordinating conjunction\n",
    "        'PUNCT',  # punctuation\n",
    "        'SYM',  # symbol\n",
    "        'X',  # other\n",
    "        )\n",
    "    ent_type = ent.ent_type_  # get entity type\n",
    "    if ent_type == '':\n",
    "        ent_type = 'NOUN_CHUNK'\n",
    "        ent = ' '.join(str(t.text) for t in\n",
    "                nlp(str(ent)) if t.pos_\n",
    "                not in unwanted_tokens and t.is_stop == False)\n",
    "    elif ent_type in ('NOMINAL', 'CARDINAL', 'ORDINAL') and str(ent).find(' ') == -1:\n",
    "        t = ''\n",
    "        for i in range(len(sent) - ent.i):\n",
    "            if ent.nbor(i).pos_ not in ('VERB', 'PUNCT'):\n",
    "                t += ' ' + str(ent.nbor(i))\n",
    "            else:\n",
    "                ent = t.strip()\n",
    "                break\n",
    "    return ent, ent_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_kg(pairs):\n",
    "    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n",
    "            create_using=nx.MultiDiGraph())\n",
    "    node_deg = nx.degree(k_graph)\n",
    "    layout = nx.spring_layout(k_graph, k=0.15, iterations=20)\n",
    "    plt.figure(num=None, figsize=(20, 5), dpi=100)\n",
    "    nx.draw_networkx(\n",
    "        k_graph,\n",
    "        node_size=[int(deg[1]) * 1000 for deg in node_deg],\n",
    "        arrowsize=20,\n",
    "        linewidths=1.5,\n",
    "        pos=layout,\n",
    "        edge_color='red',\n",
    "        edgecolors='black',\n",
    "        node_color='white',\n",
    "        )\n",
    "    labels = dict(zip(list(zip(pairs.subject, pairs.object)),\n",
    "                  pairs['relation'].tolist()))\n",
    "    nx.draw_networkx_edge_labels(k_graph, pos=layout, edge_labels=labels,\n",
    "                                 font_color='red')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "for text in full_text:\n",
    "    pairs = entity_pairs(text)\n",
    "    draw_kg(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer_v2",
   "language": "python",
   "name": "summarizer_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
